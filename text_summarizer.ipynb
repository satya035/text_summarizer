{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jba515tLfbg",
        "outputId": "dbd28360-719c-4338-80a3-ef36babd9549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2vdmql3SLSqA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "stopwords = list(STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9DNer5PsMzat"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\n",
        "NLP encompasses a variety of tasks, including:\n",
        "Text mining: Extracting information from unstructured text data.\n",
        "Machine translation: Translating text from one language to another.\n",
        "Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
        "Information retrieval: Finding relevant information from a large dataset.\n",
        "Question answering: Answering questions posed in natural language.\n",
        "Chatbots and virtual assistants: Creating conversational agents that can interact with humans.\n",
        "To achieve these tasks, NLP techniques often involve:\n",
        "\n",
        "Tokenization: Breaking text into individual words or tokens.\n",
        "Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).\n",
        "Named entity recognition: Identifying named entities like people, organizations, and locations.\n",
        "Dependency parsing: Analyzing the grammatical structure of a sentence.\n",
        "Machine learning: Using algorithms to train models on large datasets.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUASrE72LseV",
        "outputId": "7082abd4-a04a-4fd3-804c-b3d0b74e4ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'It', 'aims', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'meaningful', 'way', '.', '\\n', 'NLP', 'encompasses', 'a', 'variety', 'of', 'tasks', ',', 'including', ':', '\\n', 'Text', 'mining', ':', 'Extracting', 'information', 'from', 'unstructured', 'text', 'data', '.', '\\n', 'Machine', 'translation', ':', 'Translating', 'text', 'from', 'one', 'language', 'to', 'another', '.', '\\n', 'Sentiment', 'analysis', ':', 'Determining', 'the', 'sentiment', '(', 'positive', ',', 'negative', ',', 'or', 'neutral', ')', 'expressed', 'in', 'a', 'piece', 'of', 'text', '.', '\\n', 'Information', 'retrieval', ':', 'Finding', 'relevant', 'information', 'from', 'a', 'large', 'dataset', '.', '\\n', 'Question', 'answering', ':', 'Answering', 'questions', 'posed', 'in', 'natural', 'language', '.', '\\n', 'Chatbots', 'and', 'virtual', 'assistants', ':', 'Creating', 'conversational', 'agents', 'that', 'can', 'interact', 'with', 'humans', '.', '\\n', 'To', 'achieve', 'these', 'tasks', ',', 'NLP', 'techniques', 'often', 'involve', ':', '\\n\\n', 'Tokenization', ':', 'Breaking', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', '\\n', 'Part', '-', 'of', '-', 'speech', 'tagging', ':', 'Identifying', 'the', 'grammatical', 'category', 'of', 'each', 'word', '(', 'noun', ',', 'verb', ',', 'etc', '.', ')', '.', '\\n', 'Named', 'entity', 'recognition', ':', 'Identifying', 'named', 'entities', 'like', 'people', ',', 'organizations', ',', 'and', 'locations', '.', '\\n', 'Dependency', 'parsing', ':', 'Analyzing', 'the', 'grammatical', 'structure', 'of', 'a', 'sentence', '.', '\\n', 'Machine', 'learning', ':', 'Using', 'algorithms', 'to', 'train', 'models', 'on', 'large', 'datasets', '.']\n",
            "{'Natural': 1, 'Language': 1, 'Processing': 1, 'NLP': 3, 'field': 1, 'artificial': 1, 'intelligence': 1, 'focuses': 1, 'interaction': 1, 'computers': 2, 'human': 2, 'language': 4, 'aims': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'meaningful': 1, 'way': 1, 'encompasses': 1, 'variety': 1, 'tasks': 2, 'including': 1, 'Text': 1, 'mining': 1, 'Extracting': 1, 'information': 2, 'unstructured': 1, 'text': 4, 'data': 1, 'Machine': 2, 'translation': 1, 'Translating': 1, 'Sentiment': 1, 'analysis': 1, 'Determining': 1, 'sentiment': 1, 'positive': 1, 'negative': 1, 'neutral': 1, 'expressed': 1, 'piece': 1, 'Information': 1, 'retrieval': 1, 'Finding': 1, 'relevant': 1, 'large': 2, 'dataset': 1, 'Question': 1, 'answering': 1, 'Answering': 1, 'questions': 1, 'posed': 1, 'natural': 1, 'Chatbots': 1, 'virtual': 1, 'assistants': 1, 'Creating': 1, 'conversational': 1, 'agents': 1, 'interact': 1, 'humans': 1, 'achieve': 1, 'techniques': 1, 'involve': 1, '\\n\\n': 1, 'Tokenization': 1, 'Breaking': 1, 'individual': 1, 'words': 1, 'tokens': 1, 'speech': 1, 'tagging': 1, 'Identifying': 2, 'grammatical': 2, 'category': 1, 'word': 1, 'noun': 1, 'verb': 1, 'etc': 1, 'Named': 1, 'entity': 1, 'recognition': 1, 'named': 1, 'entities': 1, 'like': 1, 'people': 1, 'organizations': 1, 'locations': 1, 'Dependency': 1, 'parsing': 1, 'Analyzing': 1, 'structure': 1, 'sentence': 1, 'learning': 1, 'algorithms': 1, 'train': 1, 'models': 1, 'datasets': 1}\n"
          ]
        }
      ],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "punctuation = punctuation + '\\n'\n",
        "punctuation\n",
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "  if word.text.lower() not in stopwords:\n",
        "    if word.text.lower() not in punctuation:\n",
        "      if word.text not in word_frequencies.keys():\n",
        "        word_frequencies[word.text] = 1\n",
        "      else:\n",
        "        word_frequencies[word.text] += 1\n",
        "print(word_frequencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI7AltRYLs5K",
        "outputId": "e4d056e2-f3f4-42e4-c718-92ecf3c8110f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Natural': 0.25, 'Language': 0.25, 'Processing': 0.25, 'NLP': 0.75, 'field': 0.25, 'artificial': 0.25, 'intelligence': 0.25, 'focuses': 0.25, 'interaction': 0.25, 'computers': 0.5, 'human': 0.5, 'language': 1.0, 'aims': 0.25, 'enable': 0.25, 'understand': 0.25, 'interpret': 0.25, 'generate': 0.25, 'meaningful': 0.25, 'way': 0.25, 'encompasses': 0.25, 'variety': 0.25, 'tasks': 0.5, 'including': 0.25, 'Text': 0.25, 'mining': 0.25, 'Extracting': 0.25, 'information': 0.5, 'unstructured': 0.25, 'text': 1.0, 'data': 0.25, 'Machine': 0.5, 'translation': 0.25, 'Translating': 0.25, 'Sentiment': 0.25, 'analysis': 0.25, 'Determining': 0.25, 'sentiment': 0.25, 'positive': 0.25, 'negative': 0.25, 'neutral': 0.25, 'expressed': 0.25, 'piece': 0.25, 'Information': 0.25, 'retrieval': 0.25, 'Finding': 0.25, 'relevant': 0.25, 'large': 0.5, 'dataset': 0.25, 'Question': 0.25, 'answering': 0.25, 'Answering': 0.25, 'questions': 0.25, 'posed': 0.25, 'natural': 0.25, 'Chatbots': 0.25, 'virtual': 0.25, 'assistants': 0.25, 'Creating': 0.25, 'conversational': 0.25, 'agents': 0.25, 'interact': 0.25, 'humans': 0.25, 'achieve': 0.25, 'techniques': 0.25, 'involve': 0.25, '\\n\\n': 0.25, 'Tokenization': 0.25, 'Breaking': 0.25, 'individual': 0.25, 'words': 0.25, 'tokens': 0.25, 'speech': 0.25, 'tagging': 0.25, 'Identifying': 0.5, 'grammatical': 0.5, 'category': 0.25, 'word': 0.25, 'noun': 0.25, 'verb': 0.25, 'etc': 0.25, 'Named': 0.25, 'entity': 0.25, 'recognition': 0.25, 'named': 0.25, 'entities': 0.25, 'like': 0.25, 'people': 0.25, 'organizations': 0.25, 'locations': 0.25, 'Dependency': 0.25, 'parsing': 0.25, 'Analyzing': 0.25, 'structure': 0.25, 'sentence': 0.25, 'learning': 0.25, 'algorithms': 0.25, 'train': 0.25, 'models': 0.25, 'datasets': 0.25}\n",
            "[Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language., It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\n",
            ", NLP encompasses a variety of tasks, including:\n",
            "Text mining: Extracting information from unstructured text data.\n",
            ", Machine translation: Translating text from one language to another.\n",
            ", Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
            ", Information retrieval: Finding relevant information from a large dataset.\n",
            ", Question answering: Answering questions posed in natural language.\n",
            ", Chatbots and virtual assistants: Creating conversational agents that can interact with humans.\n",
            ", To achieve these tasks, NLP techniques often involve:\n",
            "\n",
            "Tokenization: Breaking text into individual words or tokens.\n",
            ", Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).\n",
            ", Named entity recognition: Identifying named entities like people, organizations, and locations.\n",
            ", Dependency parsing: Analyzing the grammatical structure of a sentence.\n",
            ", Machine learning: Using algorithms to train models on large datasets.]\n"
          ]
        }
      ],
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency\n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "print(word_frequencies)\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvQgyRrLtG0",
        "outputId": "333d86a1-d635-486f-98d5-683a5e921783"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.: 4.5,\n",
              " It aims to enable computers to understand, interpret, and generate human language in a meaningful way.: 3.75,\n",
              " NLP encompasses a variety of tasks, including:\n",
              " Text mining: Extracting information from unstructured text data.: 4.5,\n",
              " Machine translation: Translating text from one language to another.: 2.25,\n",
              " Sentiment analysis: Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.: 3.0,\n",
              " Information retrieval: Finding relevant information from a large dataset.: 2.25,\n",
              " Question answering: Answering questions posed in natural language.: 2.25,\n",
              " Chatbots and virtual assistants: Creating conversational agents that can interact with humans.: 1.5,\n",
              " To achieve these tasks, NLP techniques often involve:\n",
              " \n",
              " Tokenization: Breaking text into individual words or tokens.: 3.25,\n",
              " Part-of-speech tagging: Identifying the grammatical category of each word (noun, verb, etc.).: 2.25,\n",
              " Named entity recognition: Identifying named entities like people, organizations, and locations.: 2.25,\n",
              " Dependency parsing: Analyzing the grammatical structure of a sentence.: 1.25,\n",
              " Machine learning: Using algorithms to train models on large datasets.: 1.75}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "  for word in sent:\n",
        "    if word.text.lower() in word_frequencies.keys():\n",
        "      if sent not in sentence_scores.keys():\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "sentence_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ot5tkgufMFHH"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest\n",
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length\n",
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "summary\n",
        "final_summary = [word.text for word in summary]\n",
        "summary = ' '.join(final_summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6JmawZbMF9W",
        "outputId": "da884209-3831-42b0-97aa-6d50ae404b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "          \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language.\",\n",
            "          \"NLP encompasses a variety of tasks, including:\\nText mining: Extracting information from unstructured text data.\\n\",\n",
            "          \"It aims to enable computers to understand, interpret, and generate human language in a meaningful way.\\n\"\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "print(json.dumps(final_summary, indent=10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
